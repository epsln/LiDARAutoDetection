%Notes on Object Detection available at : https://www.mdpi.com/2072-4292/12/1/143 

This article by Qian et al. aims to solve two issues prevalent in anchor-based detection methods:
First the loss of low level information when using only the highest level feature maps for the feature extraction of region proposal.
Secondly, existing metrics, such as IoU, are not able to measure the distance between two non overlapping bounding boxes. During training, the bounding box loss is not able to directly optimize this metric. 

The authors implements a new metric, the Generalized IoU (GIoU), which is able to measure the distance between non-overlapping bounding boxes, along with a bounding box loss system that is able to directly optimize the new metric. A new multi-level feature module (MLFF), is proposed, and incorporated into an existing network.

This allows the authors to reach state of the art performance on the NWPU VHR-10 dataset\cite{nwpu}.

\subsection{Architecture}
\subsubsection{General Network Architecture}
The network can use an arbitrary size image as an input. This image is fed into a FPN, which acts as the backbone of the network. This FPN outputs multi-scale feature maps at different levels. Those multi-scale feature maps are used by the MLFF, which pools features using RoIAlign\cite{resNet} across multiple levels and concatenates them along the channel dimension. The fused features are utilized for bounding box regression and classification. The novel generalized IoU is used, instead of the smooth L1 loss.


\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{ODRSIArchi.jpg}
	\caption[General Architecture of the ODRSI]{Architecture of the proposed framework. The left part shows the feature pyramid network. Multilevel features fusion is shown in the middle, and classification and bounding box regression based on the IGIoU loss is shown in the rightmost part.}
  \label{fig:archiQian}
\end{figure}

\subsubsection{MLFF}
A novel MLFF module is proposed. The feature maps of all levels are used by a MLFF module for feature extraction. Each proposal generated by the FPN are mapped to the feature maps of all levels. The size and location of the proposed region in the feature maps can be calculated based on the size ration between the proposal and the feature maps. 

Four regions of each proposal are transformed into four groups of $7 \times 7$ feature maps, denoted $F_2, F_3, F_4$ and $F_5$ in figure~\ref{fig:archiQian} using RoiAlign\cite{resNet}. The features are then concatenated along the channel dimension into a fused feature map called $F$.

Finally, a convolutional layer with a $7 \times 7$ kernel is used on $F$ to obtain $F_{C1}$ which is then passed to a fully connected layer.


\subsection{Generalized Intersection over Union}

\begin{figure}[h!]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\textwidth]{overlappingBox.jpg}  
  \caption{Two intersecting bounding boxes}
  \label{fig:sub-first}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=.6\linewidth]{nonoverlappingBox.jpg}  
  \caption{Two non-overlapping bounding boxes}
  \label{fig:sub-second}
\end{subfigure}
	\caption{Illustration showing the two cases of bounding box position: intersecting and non-overlapping. The rectangle enclosed by a green solid line denotes the ground truth $B_{GT}$; the predicted box $B_{PT}$ is denoted by a red solid line, and the smallest enclosing box $B_{EC}$ is denoted by a blue dashed line.}
\label{fig:giou}
\end{figure}


A novel metric, the Generalized IoU (GIoU) is proposed to enhance the evaluation of proximity between two bounding boxes. Figure \ref{fig:giou} shows the difference between IoU and GIoU. The traditional IoU is insensitive to the scales of bounding boxes, and can be calculated using formula~\ref{eq:IoU}. Let $B_{GT}$ be the ground truth bounding box and $B_{PT}$ be the predicted bounding box.

\begin{equation}\label{eq:IoU}
	IoU = \frac{area(B_{GT} \cap B_{PT})}{area(B_{GT} \cup B_{PT})}
\end{equation}

The IoU is essentially the fraction of the intersection of the area of the predicted bounding box and the ground truth over the union of both bounding box. The IoU is not capable of measuring the distance when two bounding boxes are not overlapping. The introduced metric, address this issue.

The formula for the GIoU is as follows:

\begin{equation}
	%Eq (5) in the original paper may have a typo where \cup is represented as U.......
	GIoU = IoU + \frac{area(B_{GT} \cup B_{PT})}{area(B_{EC})} - 1
\end{equation}

Where $B_{EC}$ represents the smallest enclosing box of $B_{GT}$ and $B_{PT}$. The IoU is inversely proportional to he distance between $B_{GT}$ and $B_{PT}$ where they are overlapping, but stays at 0 when they were not overlapping. The GIoU is proportional to the distance of the two bounding boxes, and decreases with the distance between $B_{GT}$ and $B_{PT}$, whether or not the bounding boxes were overlapping.
\subsection{Bounding Box Regression based on Improved GIoU Loss (IGIoU)}
The bounding box regression loss used in traditional object detection methods is usually adopted to smooth the L1 or L2 loss. However, those two loss functions do not directly optimize the IoU metric. The smooth L1 or L2 loss are used to optimize the four parameters of the predicted bounding box, and the IoU is used to give more importance to the overlapping degree between the two bounding boxes. 

Integrating the value of the GIoU into the loss can be done using formula~\ref{eq:giouloss} from Rezatofighi et al. \cite{giou}.

\begin{equation}
	\label{eq:giouloss}
		L_{GIoU} = 1 - GIoU
\end{equation}

The GIoU loss has a constant gradient during the training process, which restricts the effect of bounding box regression. The authors note that strength of the training should be enhanced when the predicted bounding box is far away from the ground truth, i.e. the absolute value of the gradient should be higher when the GIoU is small. Moreover, the value of the bounding box regression loss should decrease with the GIoU. 

The improved GIoU Loss (IGIoU) is used to address those issues, and is given in the following formula:

\begin{equation}
	L_{IGIoU} = 2 \times log_2 - 2 \times log(1 + GIoU)
\end{equation}

\subsection{Results}
To validate the IGIoU loss and the MLFF module, quantitative comparisons were made between the proposed methods and five others methods on the NWPU VHR-10 dataset\cite{nwpu}. Those results are listed in table~\ref{tab:ODRSIcomparison}

Table~\ref{tab:ODRSIres} shows results of the proposed method on the DIOR\cite{dior} dataset. The DIOR dataset is a large scale benchmark, of size comparable to the DOTA dataset\cite{dota}. We see that the proposed method, with FPN+MLFF+IGIoU is superior to the baseline FPN in all of the evaluation metrics. It should be noted that the performance of FPN+MLFF+IGIoU is better than that of FPN+IGIoU and FPN+MLFF, which indicates that the MLFF in combination with IGIoU loss is effective.

The proposed method is also evaluated against four state of the art methods on the NWPU VHR-10 datasets, and are listed in table~\ref{tab:ODRSIcomparison}. 

\begin{table}[h!]
	\centering
	\begin{tabular}{@{}lllllll@{}}
		\toprule
		Method       &               & GIoU          &               &               & IoU           &               \\ \cmidrule(l){2-7} 
		             & mAP (\%)      & AP50(\%)      & AP75(\%)      & mAP(\%)       & AP50(\%)      & AP75(\%)      \\ \midrule
			     Faster R-CNN & 53.5          & 86.8          & 61.0          & 54.6          & 87.1          & 62.6          \\
			     Mask R-CNN   & 54.7          & 88.8          & 62.6          & 55.8          & 89.4          & 64.2          \\
			     FPN          & 55.3          & 88.8          & 64.0          & 56.5          & 89.3          & 65.9          \\
			     PANet        & 56.3          & \textbf{90.5} & 63.9          & 57.8          & \textbf{91.8} & 65.8          \\
			     Proposed Method& \textbf{58.0} & \textbf{90.5} & \textbf{67.5} & \textbf{59.2} & 91.4          & \textbf{69.6} \\ \bottomrule
	\end{tabular}
	\caption{Comparison of the ODRSI against four existing detection framework on the NWPU VHR-10}
	\label{tab:ODRSIcomparison}
\end{table}

The method obtains state of the art results and better precision scores than all of the other tested methods, except in one case. 

\begin{table}[h!]
	\centering
	\begin{tabular}{@{}lllllll@{}}
		\toprule
		Method       &               & GIoU          &               &               & IoU           &              \\ \cmidrule(l){2-7} 
			    & mAP (\%)      & AP50(\%)      & AP75(\%)      & mAP(\%)       & AP50(\%)      & AP75(\%)      \\ \midrule
	      FPN(baseline) & 42.6          & 66.5          & 46.3          & 43.6          & 67.9          & 47.6          \\
	      FPN + MLFF    & 43.3          & 67.8          & 46.9          & 44.2          & 68.9          & 48.1          \\
	      FPN + GIoU    & 43.3          & 66.7          & 47.5          & 44.2          & 67.9          & 48.4          \\
	      FPN + IGIoU   & 44.0          & 67.0          & 48.2          & 44.8          & 68.2          & 49.3          \\
	      FPN+MLFF+GIoU & 43.8          & 67.2          & 47.6          & 44.6          & 68.5          & 48.7          \\
		FPN+MLFF+IGIoU& \textbf{44.8} & \textbf{67.9} & \textbf{49.2} & \textbf{45.7} & \textbf{69.2}& \textbf{50.3}\\ \bottomrule
	\end{tabular}
	\caption{Comparison with the baseline method on the DIOR datasets}
	\label{tab:ODRSIres}
\end{table}
\clearpage
