\relax 
\providecommand\hyper@newdestlabel[2]{}
\abx@aux@sortscheme{none}
\abx@aux@refcontext{none/global/}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\HyPL@Entry{1<</S/D>>}
\abx@aux@cite{cutMix}
\abx@aux@segm{0}{0}{cutMix}
\abx@aux@cite{yolov4}
\abx@aux@segm{0}{0}{yolov4}
\abx@aux@cite{dropOut}
\abx@aux@segm{0}{0}{dropOut}
\abx@aux@cite{dropBlock}
\abx@aux@segm{0}{0}{dropBlock}
\abx@aux@cite{labelSmooth}
\abx@aux@segm{0}{0}{labelSmooth}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Data Augmentation}{2}{section.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Image Modification}{2}{subsection.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Regularization and Normalization}{2}{subsection.1.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dropBlockA}{{1a}{2}{Base image\relax }{figure.caption.2}{}}
\newlabel{sub@fig:dropBlockA}{{a}{2}{Base image\relax }{figure.caption.2}{}}
\newlabel{fig:dropBlockB}{{1b}{2}{Random Drop in activations\relax }{figure.caption.2}{}}
\newlabel{sub@fig:dropBlockB}{{b}{2}{Random Drop in activations\relax }{figure.caption.2}{}}
\newlabel{fig:dropBlockC}{{1c}{2}{DropBlock\relax }{figure.caption.2}{}}
\newlabel{sub@fig:dropBlockC}{{c}{2}{DropBlock\relax }{figure.caption.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The blue regions represents neurons that contains semantic information on the base image. (b) shows the effect of removing activations at random, which is not effective as neurons close to each other contains closely related semantic information. (c) shows the DropBlock method, which have a better chance of entirely removing important semantic information on the base image, such as the head or the feet of the dog, forcing the remaining neurons to learn useful features\relax }}{2}{figure.caption.2}}
\newlabel{fig:dropBlock}{{1}{2}{The blue regions represents neurons that contains semantic information on the base image. (b) shows the effect of removing activations at random, which is not effective as neurons close to each other contains closely related semantic information. (c) shows the DropBlock method, which have a better chance of entirely removing important semantic information on the base image, such as the head or the feet of the dog, forcing the remaining neurons to learn useful features\relax }{figure.caption.2}{}}
\abx@aux@cite{CSPDarknet53}
\abx@aux@segm{0}{0}{CSPDarknet53}
\abx@aux@segm{0}{0}{yolov4}
\abx@aux@cite{resNeXt}
\abx@aux@segm{0}{0}{resNeXt}
\abx@aux@cite{zhuang2019}
\abx@aux@segm{0}{0}{zhuang2019}
\abx@aux@cite{mish}
\abx@aux@segm{0}{0}{mish}
\abx@aux@cite{yolt}
\abx@aux@segm{0}{0}{yolt}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Model Architecture}{3}{section.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Backbone and activation}{3}{subsection.2.1}}
\newlabel{}{{2}{3}{}{figure.caption.3}{}}
\newlabel{fig:mish}{{3a}{3}{Mish Activation Function\relax }{figure.caption.4}{}}
\newlabel{sub@fig:mish}{{a}{3}{Mish Activation Function\relax }{figure.caption.4}{}}
\newlabel{fig:swish}{{3b}{3}{Swish Activation Function\relax }{figure.caption.4}{}}
\newlabel{sub@fig:swish}{{b}{3}{Swish Activation Function\relax }{figure.caption.4}{}}
\newlabel{fig:selu}{{3c}{3}{Scaled Exponential Linear Unit (SELU) Activation Function\relax }{figure.caption.4}{}}
\newlabel{sub@fig:selu}{{c}{3}{Scaled Exponential Linear Unit (SELU) Activation Function\relax }{figure.caption.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Activation Function Candidates\relax }}{3}{figure.caption.4}}
\newlabel{fig:activations}{{3}{3}{Activation Function Candidates\relax }{figure.caption.4}{}}
\abx@aux@segm{0}{0}{zhuang2019}
\abx@aux@cite{qianAl}
\abx@aux@segm{0}{0}{qianAl}
\abx@aux@cite{maskRCNN}
\abx@aux@segm{0}{0}{maskRCNN}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Dual Scale Detectors}{4}{subsection.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Multi-scale Feature Fusion}{4}{subsection.2.3}}
\newlabel{fig:mlffZhuang}{{4}{4}{}{figure.caption.5}{}}
\abx@aux@cite{yolov3}
\abx@aux@segm{0}{0}{yolov3}
\abx@aux@cite{yu2015}
\abx@aux@segm{0}{0}{yu2015}
\newlabel{fig:mlffQian}{{5}{5}{}{figure.caption.6}{}}
\newlabel{fig:pan}{{6a}{5}{PAN\relax }{figure.caption.7}{}}
\newlabel{sub@fig:pan}{{a}{5}{PAN\relax }{figure.caption.7}{}}
\newlabel{fig:panmod}{{6b}{5}{Modified PAN\relax }{figure.caption.7}{}}
\newlabel{sub@fig:panmod}{{b}{5}{Modified PAN\relax }{figure.caption.7}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces PAN and its YOLO modification\relax }}{5}{figure.caption.7}}
\newlabel{fig:PAN}{{6}{5}{PAN and its YOLO modification\relax }{figure.caption.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Receptive Field Improvements}{5}{subsection.2.4}}
\abx@aux@cite{sam}
\abx@aux@segm{0}{0}{sam}
\abx@aux@segm{0}{0}{yolov4}
\abx@aux@segm{0}{0}{qianAl}
\abx@aux@segm{0}{0}{yolt}
\newlabel{fig:dilConvDarknet}{{7}{6}{}{figure.caption.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Attention Modules}{6}{subsection.2.5}}
\newlabel{fig:samnorm}{{8a}{6}{SAM\relax }{figure.caption.9}{}}
\newlabel{sub@fig:samnorm}{{a}{6}{SAM\relax }{figure.caption.9}{}}
\newlabel{fig:samod}{{8b}{6}{Modified SAM\relax }{figure.caption.9}{}}
\newlabel{sub@fig:samod}{{b}{6}{Modified SAM\relax }{figure.caption.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces SAM and its YOLO modification\relax }}{6}{figure.caption.9}}
\newlabel{fig:SAM}{{8}{6}{SAM and its YOLO modification\relax }{figure.caption.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Post-processing}{6}{subsection.2.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}List of all the possible testing arrangements}{6}{subsection.2.7}}
\abx@aux@cite{ishida2020}
\abx@aux@segm{0}{0}{ishida2020}
\abx@aux@cite{mnist}
\abx@aux@segm{0}{0}{mnist}
\abx@aux@cite{fashionMNIST}
\abx@aux@segm{0}{0}{fashionMNIST}
\abx@aux@cite{cifar}
\abx@aux@segm{0}{0}{cifar}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Training and Evaluation}{8}{section.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Loss System}{8}{subsection.3.1}}
\newlabel{eq:GIoU}{{1}{8}{Loss System}{equation.3.1}{}}
\newlabel{eq:iou}{{2}{8}{Loss System}{equation.3.2}{}}
\newlabel{eq:giouLoss}{{3}{8}{Loss System}{equation.3.3}{}}
\newlabel{fig:lossA}{{9a}{8}{Without Flooding\relax }{figure.caption.10}{}}
\newlabel{sub@fig:lossA}{{a}{8}{Without Flooding\relax }{figure.caption.10}{}}
\newlabel{fig:lossB}{{9b}{8}{With Flooding\relax }{figure.caption.10}{}}
\newlabel{sub@fig:lossB}{{b}{8}{With Flooding\relax }{figure.caption.10}{}}
\newlabel{fig:lossC}{{9c}{8}{CIFAR-10 without flooding\relax }{figure.caption.10}{}}
\newlabel{sub@fig:lossC}{{c}{8}{CIFAR-10 without flooding\relax }{figure.caption.10}{}}
\newlabel{fig:lossD}{{9d}{8}{CIFAR-10 with flooding\relax }{figure.caption.10}{}}
\newlabel{sub@fig:lossD}{{d}{8}{CIFAR-10 with flooding\relax }{figure.caption.10}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Subfigure (a) shows the different regime during which a network first learns, then overfit. During [A], the network learns correctly, and both the training loss and the test loss decreases. During [B] the training loss continues to decreases, but the test loss increases; this is overfitting. In [C] the training loss is nearly zero. The authors show a way to avoid [C] by flooding the bottom area, forcing the loss to stay around a constant, which leads to a decreasing test loss. This is seen in (b) and (c) on the CIFAR-10 dataset\relax }}{8}{figure.caption.10}}
\newlabel{fig:}{{9}{8}{Subfigure (a) shows the different regime during which a network first learns, then overfit. During [A], the network learns correctly, and both the training loss and the test loss decreases. During [B] the training loss continues to decreases, but the test loss increases; this is overfitting. In [C] the training loss is nearly zero. The authors show a way to avoid [C] by flooding the bottom area, forcing the loss to stay around a constant, which leads to a decreasing test loss. This is seen in (b) and (c) on the CIFAR-10 dataset\relax }{figure.caption.10}{}}
\abx@aux@segm{0}{0}{yolov4}
